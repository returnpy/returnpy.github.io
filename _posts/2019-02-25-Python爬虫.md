---
layout: post
title: Python爬虫开发
description: 
tags: Python爬虫
---
# Python 爬虫

### urllib

urlopen Request

urllib.parse模块

该模块可以完成对url的编解码

### urlopen

可以发起一个http请求，返回一个response对象

```python
# urlopen的用法
urllib.request.urlopen(url, data=None, [timeout,]*, cafile=None, capath=None, cadefault=False, context=None)
```

两行代码爬百度首页

```python
import urllib.request

response = urllib.request.urlopen('http://www.baidu.com')
print(response.read().decode('utf-8'))
```

我的第一个POST请求

```python
import urllib.parse
import urllib.request

# 如果有request中有data数据那么就是POST请求，如果没有的话就是GET请求
data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
response = urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read())
```

测试超时时间 

不超时

```python
import urllib.request
# timeout表示设置超时间，表示如果指定时间后没有接收到response，那么抛异常timed out
response = urllib.request.urlopen('http://httpbin.org/get', timeout=10)
print(response.read())
```

超时

```python
import socket
import urllib.request
import urllib.error
# 超时时间为0.1秒,所以很容易就超时了
try:
    response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```

### 响应

响应类型

```python
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(type(response))
```

状态码、响应头、服务器

```python
import urllib.request

response = urllib.request.urlopen('http://www.python.org')
print(response.status)
print(response.getheaders())
print(response.getheader('Server'))
```

响应体

```python
import urllib.request
# 拿到以后是字节，需要转换成unicode字符
response = urllib.request.urlopen('https://www.python.org')
print(response.read().decode('utf-8'))
```

### Request

可以构造http请求头

我们需要传一个请求头过去的时候，urlopen并没有提供参数传递，所以我们就需要借助urllib发送一个request对象

```python
import urllib.request
# 我们先实例化一个request对象
request = urllib.request.Request('https://python.org')
# 然后把request对象当做参数传递给urlopen
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

我们自己构建request对象

```python
from urllib import request, parse

# 需要的user-agent
ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20'
# 请求的url
url = 'http://httpbin.org/post'
# 构造一个请求头
headers = {
    'User-agent': ua,
    'Host': 'httpbin.org'
}
# 请求参数是字典形式
dict1 = {
    'name': 'Germey'
}
# 将请求参数编码成url方式
data = bytes(parse.urlencode(dict1), encoding='utf8')
# 构建请求对象
req = request.Request(url=url, data=data, headers=headers, method='POST')
# 把对象传递给urlopen
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```

构建请求头头的方式除了上面这种以外，还提供了request.add_headers方法

```python
from urllib import request, parse

url = 'http://httpbin.org/post'
dict1 = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict1), encoding='utf8')
req = request.Request(url=url, data=data, method='POST')
# add_header方法接受
req.add_header('User-agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```

当我们需要设置代理、Cookie的时候发现Request对象也帮助不了我们，这时候就需要用到Handler

### Handler

获取cookie

```python
import http.cookiejar 
import urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name + "=" + item.value)
    
```

把cookie保存到本地

```python
import http.cookiejar
import urllib.request

filename = "cookie.txt"
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
openner = urllib.request.build_opener(handler)
response = openner.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

另一种保存cookie格式

```python
import http.cookiejar
import urllib.request

filename = "cookie1.txt"
cookie = http.cookiejar.LWPCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
openner = urllib.request.build_opener(handler)
response = openner.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

加载保存好的cookie





## BeautifulSoup

用来做HTML解析

BeautifulSoup(markup="", features=None)  

第一个参数内容，第二个解析器是谁

markup可以是文件对象或者html字符串

features指定解析器，返回一个文档对象，如果不指定默认使用Python标准库的解析器，我们推荐使用lxml，还有一种html5lib

BeautifulSoup对象代表整个文档

### Tag对象

深度优先，找第一个

- 它对应着HTML中的标签，
- 它有两个常用属性：
  - name：Tag对象的名称，就是标签名称
  - attrs：标签的属性字典
  - 多值属性，对于class属性可能有多个样式
  - 属性可以被修改删除

### NavigableString

如果只想输出标记内的文本，而不关心标记的话，就要是用NavigableString。

### 注释对象



### loop文档树



 