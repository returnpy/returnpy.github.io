---
layout: post
title: Python爬虫开发
description: 
tags: Python
---
爬虫的基本流程

数据采集

数据清洗

数据持久化

### urllib

urlopen Request

urllib.parse模块

该模块可以完成对url的编解码

### urlopen

可以发起一个http请求，返回一个response对象

```python
# urlopen的用法
urllib.request.urlopen(url, data=None, [timeout,]*, cafile=None, capath=None, cadefault=False, context=None)
```

两行代码爬百度首页

```python
import urllib.request

response = urllib.request.urlopen('http://www.baidu.com')
print(response.read().decode('utf-8'))
```

我的第一个POST请求

```python
import urllib.parse
import urllib.request

# 如果有request中有data数据那么就是POST请求，如果没有的话就是GET请求
data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
response = urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read())
```

测试超时时间 

不超时

```python
import urllib.request
# timeout表示设置超时间，表示如果指定时间后没有接收到response，那么抛异常timed out
response = urllib.request.urlopen('http://httpbin.org/get', timeout=10)
print(response.read())
```

超时

```python
import socket
import urllib.request
import urllib.error
# 超时时间为0.1秒,所以很容易就超时了
try:
    response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```

### 响应

响应类型

```python
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(type(response))
```

状态码、响应头、服务器

```python
import urllib.request

response = urllib.request.urlopen('http://www.python.org')
print(response.status)
print(response.getheaders())
print(response.getheader('Server'))
```

响应体

```python
import urllib.request
# 拿到以后是字节，需要转换成unicode字符
response = urllib.request.urlopen('https://www.python.org')
print(response.read().decode('utf-8'))
```

### Request

可以构造http请求头

我们需要传一个请求头过去的时候，urlopen并没有提供参数传递，所以我们就需要借助urllib发送一个request对象

```python
import urllib.request
# 我们先实例化一个request对象
request = urllib.request.Request('https://python.org')
# 然后把request对象当做参数传递给urlopen
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

我们自己构建request对象

```python
from urllib import request, parse

# 需要的user-agent
ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20'
# 请求的url
url = 'http://httpbin.org/post'
# 构造一个请求头
headers = {
    'User-agent': ua,
    'Host': 'httpbin.org'
}
# 请求参数是字典形式
dict1 = {
    'name': 'Germey'
}
# 将请求参数编码成url方式
data = bytes(parse.urlencode(dict1), encoding='utf8')
# 构建请求对象
req = request.Request(url=url, data=data, headers=headers, method='POST')
# 把对象传递给urlopen
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```

构建请求头头的方式除了上面这种以外，还提供了request.add_headers方法

```python
from urllib import request, parse

url = 'http://httpbin.org/post'
dict1 = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict1), encoding='utf8')
req = request.Request(url=url, data=data, method='POST')
# add_header方法接受
req.add_header('User-agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```

当我们需要设置代理、Cookie的时候发现Request对象也帮助不了我们，这时候就需要用到Handler

### Handler

获取cookie

```python
import http.cookiejar 
import urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name + "=" + item.value)
    
```

把cookie保存到本地

```python
import http.cookiejar
import urllib.request

filename = "cookie.txt"
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
openner = urllib.request.build_opener(handler)
response = openner.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

另一种保存cookie格式

```python
import http.cookiejar
import urllib.request

filename = "cookie1.txt"
cookie = http.cookiejar.LWPCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
openner = urllib.request.build_opener(handler)
response = openner.open('http://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

加载保存好的cookie





## BeautifulSoup

用来做HTML解析

BeautifulSoup(markup="", features=None)  

第一个参数内容，第二个解析器是谁

markup可以是文件对象或者html字符串

features指定解析器，返回一个文档对象，如果不指定默认使用Python标准库的解析器，我们推荐使用lxml，还有一种html5lib

BeautifulSoup对象代表整个文档

### Tag对象

深度优先，找第一个

- 它对应着HTML中的标签，
- 它有两个常用属性：
  - name：Tag对象的名称，就是标签名称
  - attrs：标签的属性字典
  - 多值属性，对于class属性可能有多个样式
  - 属性可以被修改删除

### NavigableString

如果只想输出标记内的文本，而不关心标记的话，就要是用NavigableString。

### 注释对象



### loop文档树



### Scrapy

Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。
其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。

Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下

 ![avatar](https://images2015.cnblogs.com/blog/425762/201605/425762-20160507220247421-1722096301.png)

Scrapy主要包括了以下组件：

- **引擎(Scrapy)**
  *用来处理整个系统的数据流处理, 触发事务(框架核心)*
- **调度器(Scheduler)**
  *用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址*
- **下载器(Downloader)**
  *用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)*
- **爬虫(Spiders)**
  *爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面*
- **项目管道(Pipeline)**
  *负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。*
- **下载器中间件(Downloader Middlewares)**
  *位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。*
- **爬虫中间件(Spider Middlewares)**
  *介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。*
- **调度中间件(Scheduler Middewares)**
  *介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。*